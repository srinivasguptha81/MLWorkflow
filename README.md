# ğŸ§  MLflow Multi-Model Classification Demo

This project demonstrates how to use **MLflow** to track, manage, and compare multiple machine learning models on a classification problem.

Instead of training just one model, this project trains **five different machine learning algorithms**, evaluates their performance using standard metrics, and logs everything using **MLflow** for visualization and reproducibility.

---

## ğŸ¯ Project Objective

The objectives of this project are:

âœ” Train multiple ML models  
âœ” Evaluate model performance  
âœ” Track experiments using MLflow  
âœ” Compare models visually  
âœ” Understand experiment tracking (MLOps concept)

---

## ğŸ“Š Problem Statement

We use the **Breast Cancer Wisconsin Dataset** from Scikit-learn.

**Task:**  
Predict whether a tumor is:

- **Malignant** (Cancerous)
- **Benign** (Non-cancerous)

This is a **binary classification problem**.

---

## ğŸ¤– Machine Learning Models Implemented

Five different models are trained and compared:

1ï¸âƒ£ Logistic Regression  
2ï¸âƒ£ Decision Tree Classifier  
3ï¸âƒ£ Random Forest Classifier  
4ï¸âƒ£ Support Vector Machine (SVM)  
5ï¸âƒ£ K-Nearest Neighbors (KNN)

Each model is:

âœ” Trained  
âœ” Evaluated  
âœ” Logged into MLflow  

---

## ğŸ“ˆ Evaluation Metrics

For each model, we log:

- Accuracy
- Precision
- Recall
- F1 Score

These metrics help measure classification performance.

---

## âš™ï¸ Technologies & Libraries Used

- Python 3.10
- Scikit-learn
- MLflow
- NumPy
- Conda

---

## ğŸ—ï¸ Project Structure

```
MLflow-MultiModel-Demo/
â”‚
â”œâ”€â”€ main.py              # Main script (training + MLflow logging)
â”œâ”€â”€ requirements.txt     # Dependencies
â”œâ”€â”€ README.md            # Documentation
â””â”€â”€ mlruns/              # MLflow tracking data (auto-generated)
```

---

## ğŸš€ Setup Instructions (Windows + Conda)

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/srinivasguptha81/MLWorkflow.git
```

---

### 2ï¸âƒ£ Create Conda Environment

```bash
conda create -n mlflow_env python=3.10
```

---

### 3ï¸âƒ£ Activate Environment

```bash
conda activate mlflow_env
```

---

### 4ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

(Optional if setuptools warning appears)

```bash
pip install "setuptools<81"
```

---

## â–¶ï¸ Running the Project

```bash
python main.py
```

This will:

âœ” Load dataset  
âœ” Train five models  
âœ” Compute metrics  
âœ” Log experiments into MLflow  

---

## ğŸ“‚ MLflow Tracking

After running the script, MLflow creates:

```
mlruns/
```

This folder contains:

âœ” Parameters  
âœ” Metrics  
âœ” Artifacts (trained models)  
âœ” Run metadata  

---

## ğŸŒ Launch MLflow UI

```bash
mlflow ui
```

Open in browser:

```
http://127.0.0.1:5000
```

---

## ğŸ“Š MLflow Dashboard Features

Inside MLflow UI:

âœ” View experiment runs  
âœ” Compare model metrics  
âœ” Inspect parameters  
âœ” Download saved models  

This helps identify:

ğŸ† Best model  
ğŸ“‰ Worst model  

---

## ğŸ’¡ Why MLflow is Important

In machine learning projects, models are trained multiple times with:

- Different parameters  
- Different algorithms  
- Different datasets  

Without MLflow:

âŒ Hard to track experiments  
âŒ Difficult to reproduce results  
âŒ Disorganized workflow  

With MLflow:

âœ… Structured tracking  
âœ… Easy comparison  
âœ… Reproducibility  
âœ… Model management  

---

## ğŸ§  Key Learnings

âœ” Multi-model experimentation  
âœ” Classification metrics evaluation  
âœ” Experiment tracking  
âœ” Reproducible ML workflow  
âœ” Introduction to MLOps  

---

## ğŸ“ Project Abstract

This project demonstrates MLflow-based experiment tracking by training and comparing five classification algorithms: Logistic Regression, Decision Tree, Random Forest, Support Vector Machine, and K-Nearest Neighbors. The Breast Cancer dataset is used for binary classification. Performance metrics including Accuracy, Precision, Recall, and F1 Score are logged into MLflow. The project highlights reproducibility, experiment management, and model comparison.

---

## ğŸ Conclusion

This project provides practical exposure to:

ğŸ‘‰ Machine Learning Model Comparison  
ğŸ‘‰ Experiment Tracking with MLflow  
ğŸ‘‰ Reproducible ML Pipelines  

It simulates how ML experiments are managed in real-world production environments.

---

